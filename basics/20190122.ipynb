{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('abcnews-date-text.csv', error_bad_lines=False);\n",
    "data_text = data[['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1103663"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.info of                                              headline_text    index\n",
      "0        aba decides against community broadcasting lic...        0\n",
      "1           act fire witnesses must be aware of defamation        1\n",
      "2           a g calls for infrastructure protection summit        2\n",
      "3                 air nz staff in aust strike for pay rise        3\n",
      "4            air nz strike to affect australian travellers        4\n",
      "5                        ambitious olsson wins triple jump        5\n",
      "6               antic delighted with record breaking barca        6\n",
      "7        aussie qualifier stosur wastes four memphis match        7\n",
      "8             aust addresses un security council over iraq        8\n",
      "9               australia is locked into war timetable opp        9\n",
      "10       australia to contribute 10 million in aid to iraq       10\n",
      "11       barca take record as robson celebrates birthda...       11\n",
      "12                              bathhouse plans move ahead       12\n",
      "13           big hopes for launceston cycling championship       13\n",
      "14                  big plan to boost paroo water supplies       14\n",
      "15                  blizzard buries united states in bills       15\n",
      "16          brigadier dismisses reports troops harassed in       16\n",
      "17          british combat troops arriving daily in kuwait       17\n",
      "18              bryant leads lakers to double overtime win       18\n",
      "19                bushfire victims urged to see centrelink       19\n",
      "20         businesses should prepare for terrorist attacks       20\n",
      "21         calleri avenges final defeat to eliminate massu       21\n",
      "22                 call for ethanol blend fuel to go ahead       22\n",
      "23                  carews freak goal leaves roma in ruins       23\n",
      "24                            cemeteries miss out on funds       24\n",
      "25       code of conduct toughens organ donation regula...       25\n",
      "26            commonwealth bank cuts fixed home loan rates       26\n",
      "27                  community urged to help homeless youth       27\n",
      "28        council chief executive fails to secure position       28\n",
      "29         councillor to contest wollongong as independent       29\n",
      "...                                                    ...      ...\n",
      "1103633        nepal bans solo climbers from mount everest  1103633\n",
      "1103634     new years eve 2018 celebrated around australia  1103634\n",
      "1103635  new years eve australia prepares to bring in 2018  1103635\n",
      "1103636        new years eve celebrations around the world  1103636\n",
      "1103637  new years texting data load to surge as clock ...  1103637\n",
      "1103638     north korea leader kim jong un watches concert  1103638\n",
      "1103639   now its real tourists converge on sydney harbour  1103639\n",
      "1103640  nye guide for sydney best venues public transp...  1103640\n",
      "1103641    police confirm deaths of six people in seaplane  1103641\n",
      "1103642  police officer brett forte; killed in a shooti...  1103642\n",
      "1103643     p plate driver caught 100 kph over speed limit  1103643\n",
      "1103644         protesters throw rocks at police in tehran  1103644\n",
      "1103645          remembering australian lives lost in 2017  1103645\n",
      "1103646  remount horsemanship helping veterans through ...  1103646\n",
      "1103647  roger federer rivals battling injury ahead aus...  1103647\n",
      "1103648  russian tankers fuelled north korea via transf...  1103648\n",
      "1103649  sa transport department defends major intersec...  1103649\n",
      "1103650  sea plane has crashed into the hawkesbury rive...  1103650\n",
      "1103651  search for survivors in hawkesbury sea plane c...  1103651\n",
      "1103652  second sexual assault reported at falls festiv...  1103652\n",
      "1103653  severe storms forecast for nye in south east q...  1103653\n",
      "1103654  snake catcher pleads for people not to kill re...  1103654\n",
      "1103655  south australia prepares for party to welcome ...  1103655\n",
      "1103656  strikers cool off the heat with big win in ade...  1103656\n",
      "1103657    stunning images from the sydney to hobart yacht  1103657\n",
      "1103658  the ashes smiths warners near miss liven up bo...  1103658\n",
      "1103659            timelapse: brisbanes new year fireworks  1103659\n",
      "1103660           what 2017 meant to the kids of australia  1103660\n",
      "1103661   what the papodopoulos meeting may mean for ausus  1103661\n",
      "1103662  who is george papadopoulos the former trump ca...  1103662\n",
      "\n",
      "[1103663 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "#documents[:5]\n",
    "print(documents.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sgiraldo\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sgiraldo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatize example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mother in law\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize('mother in law', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemmer Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mules</td>\n",
       "      <td>mule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sized</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stating</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>siezing</td>\n",
       "      <td>siez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>itemization</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>reference</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>colonizer</td>\n",
       "      <td>colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>plotted</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word stemmed\n",
       "0       caresses  caress\n",
       "1          flies     fli\n",
       "2           dies     die\n",
       "3          mules    mule\n",
       "4         denied    deni\n",
       "5           died     die\n",
       "6         agreed    agre\n",
       "7          owned     own\n",
       "8        humbled   humbl\n",
       "9          sized    size\n",
       "10       meeting    meet\n",
       "11       stating   state\n",
       "12       siezing    siez\n",
       "13   itemization    item\n",
       "14   sensational  sensat\n",
       "15   traditional  tradit\n",
       "16     reference   refer\n",
       "17     colonizer   colon\n",
       "18       plotted    plot"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "pd.DataFrame(data = {'original word': original_words, 'stemmed': singles})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['ratepayers', 'group', 'wants', 'compulsory', 'local', 'govt', 'voting']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['ratepay', 'group', 'want', 'compulsori', 'local', 'govt', 'vote']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 4311].values[0][0]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents['headline_text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            [decid, communiti, broadcast, licenc]\n",
       "1                               [wit, awar, defam]\n",
       "2           [call, infrastructur, protect, summit]\n",
       "3                      [staff, aust, strike, rise]\n",
       "4             [strike, affect, australian, travel]\n",
       "5               [ambiti, olsson, win, tripl, jump]\n",
       "6           [antic, delight, record, break, barca]\n",
       "7    [aussi, qualifi, stosur, wast, memphi, match]\n",
       "8            [aust, address, secur, council, iraq]\n",
       "9                         [australia, lock, timet]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 communiti\n",
      "2 decid\n",
      "3 licenc\n",
      "4 awar\n",
      "5 defam\n",
      "6 wit\n",
      "7 call\n",
      "8 infrastructur\n",
      "9 protect\n",
      "10 summit\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(161, 1), (239, 1), (291, 1), (588, 1), (836, 1), (3549, 1), (3550, 1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4311]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 76 (\"bushfir\") appears 1 time.\n",
      "Word 112 (\"help\") appears 1 time.\n",
      "Word 483 (\"rain\") appears 1 time.\n",
      "Word 4014 (\"dampen\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                                     dictionary[bow_doc_4310[i][0]], \n",
    "                                                     bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5892908867507543),\n",
      " (1, 0.38929654337861147),\n",
      " (2, 0.4964985175717023),\n",
      " (3, 0.5046520327464028)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.029*\"elect\" + 0.019*\"say\" + 0.017*\"hospit\" + 0.016*\"tasmanian\" + 0.015*\"labor\" + 0.013*\"deal\" + 0.013*\"china\" + 0.011*\"polit\" + 0.011*\"talk\" + 0.011*\"vote\"\n",
      "Topic: 1 \n",
      "Words: 0.019*\"nation\" + 0.017*\"coast\" + 0.016*\"help\" + 0.015*\"countri\" + 0.015*\"state\" + 0.013*\"tasmania\" + 0.013*\"hour\" + 0.012*\"health\" + 0.012*\"indigen\" + 0.012*\"water\"\n",
      "Topic: 2 \n",
      "Words: 0.020*\"canberra\" + 0.018*\"market\" + 0.014*\"rise\" + 0.014*\"turnbul\" + 0.013*\"australian\" + 0.013*\"share\" + 0.013*\"price\" + 0.013*\"break\" + 0.011*\"bank\" + 0.010*\"week\"\n",
      "Topic: 3 \n",
      "Words: 0.062*\"polic\" + 0.023*\"crash\" + 0.019*\"interview\" + 0.018*\"miss\" + 0.018*\"shoot\" + 0.015*\"investig\" + 0.013*\"driver\" + 0.013*\"arrest\" + 0.012*\"hous\" + 0.011*\"search\"\n",
      "Topic: 4 \n",
      "Words: 0.028*\"charg\" + 0.027*\"court\" + 0.021*\"murder\" + 0.018*\"woman\" + 0.017*\"face\" + 0.016*\"die\" + 0.015*\"alleg\" + 0.015*\"brisban\" + 0.015*\"live\" + 0.014*\"jail\"\n",
      "Topic: 5 \n",
      "Words: 0.035*\"australia\" + 0.021*\"melbourn\" + 0.021*\"world\" + 0.016*\"open\" + 0.014*\"final\" + 0.013*\"donald\" + 0.011*\"sydney\" + 0.010*\"leagu\" + 0.010*\"take\" + 0.010*\"road\"\n",
      "Topic: 6 \n",
      "Words: 0.027*\"south\" + 0.026*\"kill\" + 0.015*\"island\" + 0.014*\"fall\" + 0.010*\"forc\" + 0.009*\"shark\" + 0.009*\"attack\" + 0.009*\"east\" + 0.008*\"northern\" + 0.007*\"great\"\n",
      "Topic: 7 \n",
      "Words: 0.016*\"power\" + 0.014*\"farmer\" + 0.012*\"guilti\" + 0.011*\"region\" + 0.009*\"look\" + 0.009*\"energi\" + 0.009*\"victoria\" + 0.009*\"council\" + 0.009*\"christma\" + 0.009*\"trade\"\n",
      "Topic: 8 \n",
      "Words: 0.035*\"trump\" + 0.031*\"australian\" + 0.019*\"queensland\" + 0.013*\"leav\" + 0.013*\"australia\" + 0.013*\"say\" + 0.011*\"show\" + 0.010*\"royal\" + 0.010*\"game\" + 0.009*\"meet\"\n",
      "Topic: 9 \n",
      "Words: 0.036*\"govern\" + 0.020*\"test\" + 0.018*\"rural\" + 0.013*\"worker\" + 0.012*\"news\" + 0.012*\"budget\" + 0.011*\"busi\" + 0.011*\"say\" + 0.010*\"violenc\" + 0.010*\"school\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Can you distinguish different topics using the words in each topic and their corresponding weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda_model_tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-35b888c55b3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlda_model_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Topic: {} Word: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lda_model_tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rain', 'help', 'dampen', 'bushfir']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.41997694969177246\t \n",
      "Topic: 0.017*\"attack\" + 0.016*\"kill\" + 0.012*\"victim\" + 0.012*\"violenc\" + 0.010*\"hobart\" + 0.010*\"rugbi\" + 0.010*\"secur\" + 0.010*\"say\" + 0.009*\"state\" + 0.008*\"domest\"\n",
      "\n",
      "Score: 0.21999986469745636\t \n",
      "Topic: 0.023*\"world\" + 0.014*\"final\" + 0.013*\"record\" + 0.012*\"break\" + 0.011*\"lose\" + 0.011*\"australian\" + 0.011*\"leagu\" + 0.011*\"test\" + 0.010*\"australia\" + 0.010*\"hill\"\n",
      "\n",
      "Score: 0.21999594569206238\t \n",
      "Topic: 0.027*\"south\" + 0.024*\"year\" + 0.020*\"interview\" + 0.020*\"north\" + 0.019*\"jail\" + 0.018*\"west\" + 0.014*\"island\" + 0.013*\"australia\" + 0.013*\"victoria\" + 0.010*\"china\"\n",
      "\n",
      "Score: 0.020009687170386314\t \n",
      "Topic: 0.018*\"rural\" + 0.018*\"council\" + 0.015*\"fund\" + 0.014*\"plan\" + 0.013*\"health\" + 0.012*\"chang\" + 0.011*\"nation\" + 0.010*\"price\" + 0.010*\"servic\" + 0.009*\"say\"\n",
      "\n",
      "Score: 0.020008400082588196\t \n",
      "Topic: 0.024*\"countri\" + 0.021*\"hour\" + 0.020*\"australian\" + 0.019*\"warn\" + 0.016*\"live\" + 0.013*\"indigen\" + 0.011*\"call\" + 0.009*\"victorian\" + 0.009*\"campaign\" + 0.008*\"show\"\n",
      "\n",
      "Score: 0.02000494673848152\t \n",
      "Topic: 0.031*\"queensland\" + 0.029*\"melbourn\" + 0.018*\"water\" + 0.017*\"claim\" + 0.013*\"hunter\" + 0.012*\"green\" + 0.012*\"resid\" + 0.011*\"darwin\" + 0.010*\"young\" + 0.009*\"plead\"\n",
      "\n",
      "Score: 0.020004209131002426\t \n",
      "Topic: 0.052*\"polic\" + 0.020*\"crash\" + 0.019*\"death\" + 0.017*\"sydney\" + 0.016*\"miss\" + 0.016*\"woman\" + 0.015*\"die\" + 0.015*\"charg\" + 0.014*\"shoot\" + 0.013*\"arrest\"\n",
      "\n",
      "Score: 0.019999999552965164\t \n",
      "Topic: 0.035*\"govern\" + 0.024*\"open\" + 0.018*\"coast\" + 0.017*\"tasmanian\" + 0.017*\"gold\" + 0.014*\"australia\" + 0.013*\"beat\" + 0.010*\"win\" + 0.010*\"ahead\" + 0.009*\"shark\"\n",
      "\n",
      "Score: 0.019999999552965164\t \n",
      "Topic: 0.025*\"elect\" + 0.022*\"adelaid\" + 0.012*\"perth\" + 0.011*\"take\" + 0.011*\"say\" + 0.010*\"labor\" + 0.010*\"turnbul\" + 0.009*\"vote\" + 0.009*\"royal\" + 0.009*\"time\"\n",
      "\n",
      "Score: 0.019999999552965164\t \n",
      "Topic: 0.032*\"court\" + 0.022*\"face\" + 0.020*\"charg\" + 0.020*\"home\" + 0.018*\"tasmania\" + 0.017*\"murder\" + 0.015*\"trial\" + 0.012*\"accus\" + 0.012*\"abus\" + 0.012*\"child\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test document has the highest probability to be part of the topic on the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.44014573097229004\t \n",
      "Topic: 0.014*\"south\" + 0.009*\"weather\" + 0.009*\"north\" + 0.008*\"west\" + 0.008*\"coast\" + 0.008*\"australia\" + 0.006*\"east\" + 0.006*\"queensland\" + 0.006*\"storm\" + 0.005*\"season\"\n",
      "\n",
      "Score: 0.3998423218727112\t \n",
      "Topic: 0.023*\"rural\" + 0.018*\"govern\" + 0.013*\"news\" + 0.012*\"podcast\" + 0.008*\"grandstand\" + 0.008*\"health\" + 0.007*\"budget\" + 0.007*\"busi\" + 0.007*\"nation\" + 0.007*\"fund\"\n",
      "\n",
      "Score: 0.02000250481069088\t \n",
      "Topic: 0.006*\"action\" + 0.006*\"violenc\" + 0.006*\"thursday\" + 0.005*\"domest\" + 0.005*\"cancer\" + 0.005*\"legal\" + 0.005*\"union\" + 0.005*\"breakfast\" + 0.005*\"school\" + 0.004*\"student\"\n",
      "\n",
      "Score: 0.020002111792564392\t \n",
      "Topic: 0.008*\"drum\" + 0.007*\"abbott\" + 0.007*\"farm\" + 0.006*\"dairi\" + 0.006*\"asylum\" + 0.006*\"tuesday\" + 0.006*\"water\" + 0.006*\"labor\" + 0.006*\"say\" + 0.005*\"plan\"\n",
      "\n",
      "Score: 0.020001791417598724\t \n",
      "Topic: 0.030*\"countri\" + 0.028*\"hour\" + 0.009*\"sport\" + 0.008*\"septemb\" + 0.008*\"wednesday\" + 0.007*\"commiss\" + 0.006*\"royal\" + 0.006*\"updat\" + 0.006*\"station\" + 0.005*\"bendigo\"\n",
      "\n",
      "Score: 0.02000163123011589\t \n",
      "Topic: 0.008*\"octob\" + 0.006*\"search\" + 0.006*\"miss\" + 0.006*\"inquest\" + 0.005*\"stori\" + 0.005*\"jam\" + 0.004*\"john\" + 0.004*\"harvest\" + 0.004*\"australia\" + 0.004*\"world\"\n",
      "\n",
      "Score: 0.020001478493213654\t \n",
      "Topic: 0.008*\"monday\" + 0.008*\"august\" + 0.006*\"babi\" + 0.005*\"shorten\" + 0.005*\"hobart\" + 0.004*\"victorian\" + 0.004*\"donald\" + 0.004*\"safe\" + 0.004*\"scott\" + 0.004*\"donat\"\n",
      "\n",
      "Score: 0.02000102959573269\t \n",
      "Topic: 0.017*\"charg\" + 0.014*\"murder\" + 0.011*\"court\" + 0.011*\"polic\" + 0.009*\"woman\" + 0.008*\"assault\" + 0.008*\"jail\" + 0.008*\"alleg\" + 0.007*\"accus\" + 0.007*\"guilti\"\n",
      "\n",
      "Score: 0.020000804215669632\t \n",
      "Topic: 0.022*\"interview\" + 0.013*\"market\" + 0.009*\"share\" + 0.008*\"cattl\" + 0.008*\"trump\" + 0.008*\"turnbul\" + 0.007*\"novemb\" + 0.007*\"michael\" + 0.006*\"australian\" + 0.006*\"export\"\n",
      "\n",
      "Score: 0.020000625401735306\t \n",
      "Topic: 0.019*\"crash\" + 0.014*\"kill\" + 0.009*\"fatal\" + 0.009*\"dead\" + 0.007*\"die\" + 0.007*\"truck\" + 0.007*\"polic\" + 0.006*\"attack\" + 0.006*\"injur\" + 0.006*\"bomb\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test document has the highest probability to be part of the topic on the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model on unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.3500000238418579\t Topic: 0.017*\"attack\" + 0.016*\"kill\" + 0.012*\"victim\" + 0.012*\"violenc\" + 0.010*\"hobart\"\n",
      "Score: 0.34998345375061035\t Topic: 0.025*\"elect\" + 0.022*\"adelaid\" + 0.012*\"perth\" + 0.011*\"take\" + 0.011*\"say\"\n",
      "Score: 0.18333327770233154\t Topic: 0.052*\"polic\" + 0.020*\"crash\" + 0.019*\"death\" + 0.017*\"sydney\" + 0.016*\"miss\"\n",
      "Score: 0.01667369343340397\t Topic: 0.035*\"govern\" + 0.024*\"open\" + 0.018*\"coast\" + 0.017*\"tasmanian\" + 0.017*\"gold\"\n",
      "Score: 0.01666990853846073\t Topic: 0.027*\"south\" + 0.024*\"year\" + 0.020*\"interview\" + 0.020*\"north\" + 0.019*\"jail\"\n",
      "Score: 0.016669215634465218\t Topic: 0.018*\"rural\" + 0.018*\"council\" + 0.015*\"fund\" + 0.014*\"plan\" + 0.013*\"health\"\n",
      "Score: 0.016668815165758133\t Topic: 0.024*\"countri\" + 0.021*\"hour\" + 0.020*\"australian\" + 0.019*\"warn\" + 0.016*\"live\"\n",
      "Score: 0.01666823774576187\t Topic: 0.031*\"queensland\" + 0.029*\"melbourn\" + 0.018*\"water\" + 0.017*\"claim\" + 0.013*\"hunter\"\n",
      "Score: 0.016666674986481667\t Topic: 0.032*\"court\" + 0.022*\"face\" + 0.020*\"charg\" + 0.020*\"home\" + 0.018*\"tasmania\"\n",
      "Score: 0.01666666753590107\t Topic: 0.023*\"world\" + 0.014*\"final\" + 0.013*\"record\" + 0.012*\"break\" + 0.011*\"lose\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
