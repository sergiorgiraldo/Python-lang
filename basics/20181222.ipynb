{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Create X from the features\n",
    "X = iris.data\n",
    "\n",
    "# Create y from output\n",
    "y = iris.target\n",
    "\n",
    "# Remake the variable, keeping all data where the category is not 2.\n",
    "X = X[y != 2]\n",
    "y = y[y != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the features\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the target data\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into test and training sets, with 30% of samples being put into the test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Create a scaler object\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "\n",
    "# Apply the scaler to the test data\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Logistic Regression With A L1 Penalty With Various Regularization Strengths\n",
    "\n",
    "The usefulness of L1 is that it can push feature coefficients to 0, creating a method for feature selection. In the code below we run a logistic regression with a L1 penalty four times, each time decreasing the value of C. We should expect that as C decreases, more coefficients become 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 10\n",
      "Coefficient of each feature: [[-0.08923723 -3.74927598  4.40654251  0.        ]]\n",
      "Training accuracy: 1.0\n",
      "Test accuracy: 1.0\n",
      "\n",
      "C: 1\n",
      "Coefficient of each feature: [[ 0.         -2.28814193  2.57637744  0.        ]]\n",
      "Training accuracy: 1.0\n",
      "Test accuracy: 1.0\n",
      "\n",
      "C: 0.1\n",
      "Coefficient of each feature: [[ 0.         -0.82314381  0.9717413   0.        ]]\n",
      "Training accuracy: 1.0\n",
      "Test accuracy: 1.0\n",
      "\n",
      "C: 0.01\n",
      "Coefficient of each feature: [[0. 0. 0. 0.]]\n",
      "Training accuracy: 0.5\n",
      "Test accuracy: 0.5\n",
      "\n",
      "C: 0.001\n",
      "Coefficient of each feature: [[0. 0. 0. 0.]]\n",
      "Training accuracy: 0.5\n",
      "Test accuracy: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "C = [10, 1, .1, 0.01, 0.001]\n",
    "\n",
    "for c in C:\n",
    "    clf = LogisticRegression(penalty='l1', C=c)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('C:', c)\n",
    "    print('Coefficient of each feature:', clf.coef_)\n",
    "    print('Training accuracy:', clf.score(X_train, y_train))\n",
    "    print('Test accuracy:', clf.score(X_test, y_test))\n",
    "    print('')\n",
    "    \n",
    "#Notice that as C decreases the model coefficients become smaller \n",
    "#(for example from 4.40654251 when C=10 to 0.9717413 when C=0.1), until at C=0.01 all the coefficients are zero. \n",
    "#This is the effect of the regularization penalty becoming more prominent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now with l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 10\n",
      "Coefficient of each feature: [[-0.66337383 -2.04002008  3.30348572  1.57962787]]\n",
      "Training accuracy: 1.0\n",
      "Test accuracy: 1.0\n",
      "\n",
      "C: 1\n",
      "Coefficient of each feature: [[-0.40665822 -1.29733138  2.07419066  0.94546149]]\n",
      "Training accuracy: 1.0\n",
      "Test accuracy: 1.0\n",
      "\n",
      "C: 0.1\n",
      "Coefficient of each feature: [[-0.19365102 -0.64295103  1.03105806  0.45490726]]\n",
      "Training accuracy: 1.0\n",
      "Test accuracy: 1.0\n",
      "\n",
      "C: 0.01\n",
      "Coefficient of each feature: [[-0.03813058 -0.1670509   0.2864821   0.12332521]]\n",
      "Training accuracy: 1.0\n",
      "Test accuracy: 1.0\n",
      "\n",
      "C: 0.001\n",
      "Coefficient of each feature: [[ 0.00414666 -0.01644721  0.04171045  0.01717279]]\n",
      "Training accuracy: 0.5\n",
      "Test accuracy: 0.5\n",
      "\n",
      "C: 0.0001\n",
      "Coefficient of each feature: [[ 0.00128083 -0.00122067  0.00476518  0.00190076]]\n",
      "Training accuracy: 0.5\n",
      "Test accuracy: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "C = [10, 1, .1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "for c in C:\n",
    "    clf = LogisticRegression(penalty='l2', C=c)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('C:', c)\n",
    "    print('Coefficient of each feature:', clf.coef_)\n",
    "    print('Training accuracy:', clf.score(X_train, y_train))\n",
    "    print('Test accuracy:', clf.score(X_test, y_test))\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
